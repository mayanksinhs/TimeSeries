# -*- coding: utf-8 -*-
"""Assignment.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1yrTh98wH5EBw0FY36Y6scgeU6zZlwRjp
"""

pip install pmdarima

import numpy as np
import pandas as pd
import seaborn as sns
from matplotlib import pyplot as plt

df = pd.read_excel('/content/Random Price Dataset Trial Assignment.xlsx')

df = df[['Date','Domestic Market (Contract) Blow Molding, Low']]

print(df.head())
print(df.tail())

df.head()

df = df.set_index('Date')
df.head(20)

df.info()

df.plot(figsize=(20,8))
plt.grid();

round(df.describe(),3)

from    statsmodels.tsa.seasonal import   seasonal_decompose

decomposition = seasonal_decompose(df,model='multiplicative')
decomposition.plot()

train    =   df[0:int(len(df)*0.7)] 
test     =   df[int(len(df)*0.7):]

print(train.shape)
print(test.shape)

print('First few rows of Training Data','\n',train.head(),'\n')
print('Last few rows of Training Data','\n',train.tail(),'\n')
print('First few rows of Test Data','\n',test.head(),'\n')
print('Last few rows of Test Data','\n',test.tail(),'\n')

train['Domestic Market (Contract) Blow Molding, Low'].plot(figsize=(13,5), fontsize=14)
test['Domestic Market (Contract) Blow Molding, Low'].plot(figsize=(13,5), fontsize=14)
plt.grid()
plt.legend(['Training Data','Test Data'])
plt.show()

from statsmodels.tsa.stattools import adfuller

result = adfuller(df)

# print the results
print('ADF Statistic: %f' % result[0])
print('p-value: %f' % result[1])
print('Critical Values:')
for key, value in result[4].items():
    print('\t%s: %.3f' % (key, value))

import matplotlib.pyplot as plt
from statsmodels.graphics.tsaplots import plot_acf, plot_pacf

# plot the autocorrelation function
plot_acf(df, lags=20)
plt.show()

# plot the partial autocorrelation function
plot_pacf(df, lags=20)
plt.show()

# calculate the first-order difference
diff_df = df.diff().dropna()

result = adfuller(diff_df)

# print the results
print('ADF Statistic: %f' % result[0])
print('p-value: %f' % result[1])
print('Critical Values:')
for key, value in result[4].items():
    print('\t%s: %.3f' % (key, value))

# plot the autocorrelation function
plot_acf(diff_df, lags=20)
plt.show()

# plot the partial autocorrelation function
plot_pacf(diff_df, lags=20)
plt.show()

!pip install pmdarima

from pmdarima.arima import auto_arima

# fit an auto_arima model
model = auto_arima(df, seasonal=False, trace=True)

# print the model summary
print(model.summary())

from statsmodels.tsa.arima.model import ARIMA

# fit an ARIMA model
model = ARIMA(df, order=(2,1,1))
results = model.fit()

# forecast future values
forecast = results.forecast(steps=12)

# plot the forecasted values
plt.plot(df)
plt.plot(forecast, color='red')
plt.show()

"""# **LSTM**"""

df2 = pd.read_excel('/content/Random Price Dataset Trial Assignment.xlsx', index_col='Date', parse_dates = True)
df2 = df.dropna()

import matplotlib.pyplot as pltb

df2.head(5)

df2.plot(figsize=(12,6))

result = seasonal_decompose(df2,model='multiplicative')
result.plot()

len(df2)

train = df2.iloc[:138]
test = df2.iloc[138:]

from sklearn.preprocessing import MinMaxScaler
scaler = MinMaxScaler()

scaler.fit(train)
scaled_train = scaler.transform(train)
scaled_test = scaler.transform(test)

scaled_train[:10]

from keras.preprocessing.sequence import TimeseriesGenerator

# Define Generator
  n_input = 3
  n_features = 1
  generator = TimeseriesGenerator(scaled_train, scaled_test, length=n_input, batch_size=1)

X,y = generator[0]
print(f'Given the array: \n{X.flatten()}')
print(f'Predict y: \n{y}')

X.shape

# We will generate 10 Month
n_input = 12
generator = TimeseriesGenerator(scaled_train, scaled_test, length=n_input, batch_size=1)

from keras.models import Sequential
from keras.layers import Dense
from keras.layers import LSTM

# Model
model = Sequential()
model.add(LSTM(100, activation='relu', input_shape=(n_input, n_features)))
model.add(Dense(1))
model.compile(optimizer='adam', loss='mse')

model.summary()

# Fit Model
model.fit(generator, epochs=20)

loss_per_epoch = model.history.history['loss']
plt.plot(range(len(loss_per_epoch)), loss_per_epoch)

last_train_batch = scaled_train[-12:]

last_train_batch = last_train_batch.reshape((1, n_input, n_features))

model.predict(last_train_batch)

scaled_test[0]

test_prediction = []

first_ever_batch = scaled_train[-n_input:]  # Get the last n_input values from the training data
current_batch = np.reshape(first_ever_batch, (1, n_input, n_features))

for i in range(len(test)):
    # Get the Prediction value for the current batch
    current_pred = model.predict(current_batch)[0]
    # Append the prediction into the array
    test_prediction.append(current_pred)
    # Update the batch by removing the first value and adding the current prediction
    current_batch = np.concatenate((current_batch[:, 1:, :], np.reshape(current_pred, (1, 1, n_features))), axis=1)

test_prediction

test.head()

actual_prediction = scaler.inverse_transform(test_prediction)

test['Prediction'] = actual_prediction

test.plot(figsize=(12,6))

from sklearn.metrics import mean_squared_error
from math import sqrt
rmse =sqrt(mean_squared_error(test['Domestic Market (Contract) Blow Molding, Low'],test['Prediction']))
print(rmse)